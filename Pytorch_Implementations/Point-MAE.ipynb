{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# table, car, chair, airplane, sofa, rifle(replace this category with something else), lamp\n",
    "# Potential todo : apply certain transformations like : rotations, flip operations etc.\n",
    "\n",
    "# Use separate position encoder for both encoder and decoders\n",
    "# TODO : Multi-scale clouds\n",
    "\n",
    "# TODO : Change the patch embedding for visible token to Mini-PointNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Point-MAE (WIP)\n",
    "\n",
    "Implementation of the paper Point-MAE :- https://arxiv.org/abs/2203.06604\n",
    "\n",
    "Note :- This code expects point clouds in a h5py with the group structure f --> data --> Category (eg. Table / Chair) etc from the **ShapeNet** data. You'll find a sample version of data in the data folder for reference. Once the data is read, one might see samples like the one below visualized using package **k3d**.\n",
    "\n",
    "<img data-align=\"center\" src=\"./Screenshot%202024-01-17%20at%2015.06.53.png\" width=\"650\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torchinfo import summary\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "from sys import platform\n",
    "from torch_geometric.nn import fps, knn\n",
    "\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import h5py\n",
    "import k3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_patching(original, res2):\n",
    "    plot = k3d.plot()\n",
    "    rand_item = np.random.choice(np.arange(len(original)))\n",
    "    # Generate some random colors for visualization\n",
    "    colors = [\n",
    "        0xbd72b5, 0x64cb1e, 0x489d8d, 0xb65df1, 0x3f6a69, 0x0708d2, 0xad48d2, 0x2b440f, 0xbc0b26, 0xae6c42,\\\n",
    "    ]*10\n",
    "    np.random.shuffle(colors)\n",
    "\n",
    "    pt_samples = k3d.points(positions=original[rand_item], point_size=0.008, shader='3d',color=0x3f6bc5)\n",
    "    plot += pt_samples\n",
    "\n",
    "    for x, c in zip(res2[rand_item], colors[:len(res2)]):\n",
    "        plot += k3d.points(positions=x.numpy()+0.6, point_size=0.008, shader='3d',color=c)\n",
    "\n",
    "    plot.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_PATCHES = 96\n",
    "CLOUD_POINTS = 4096\n",
    "FPS_RATIO = round(NUM_PATCHES/CLOUD_POINTS, 5)\n",
    "K_NN_K = 32\n",
    "BATCH_SIZE = 36\n",
    "MASK_RATIO = 0.3\n",
    "EMBEDDING_DIM = 384 # For each single patch\n",
    "\n",
    "MLP_UNITS = EMBEDDING_DIM * 2 # for the feed-foward network in Encoder/Decoders\n",
    "NUM_HEADS = 6\n",
    "ENCODER_DEPTH = 12\n",
    "\n",
    "EPOCHS = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device in use : mps\n"
     ]
    }
   ],
   "source": [
    "device = \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "print(\"Device in use :\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_chamfer_dist(pc1, pc2, batch=True):\n",
    "    if platform == \"darwin\":\n",
    "        pc1 = pc1.to(\"cpu\")\n",
    "        pc2 = pc2.to(\"cpu\")\n",
    "\n",
    "    assert pc1.size() == pc2.size()\n",
    "    if batch:\n",
    "        distances = []\n",
    "        for p1, p2 in zip(pc1, pc2):\n",
    "            d = torch.cdist(p1,p2).min(1).values.mean() + torch.cdist(p2,p1).min(1).values.mean()\n",
    "            distances.append(d)\n",
    "        return torch.stack(distances).sum()\n",
    "            \n",
    "    return torch.cdist(pc1,pc2).min(1).values.mean() + torch.cdist(pc2,pc1).min(1).values.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FPS(nn.Module):\n",
    "    \"\"\"\n",
    "        Perform FPS sampling on batch of point cloud items.\n",
    "    \"\"\"\n",
    "    def __init__(self, ratio):\n",
    "        super().__init__()\n",
    "        assert ratio > 0 and ratio <= 1\n",
    "        self.ratio = ratio\n",
    "    \n",
    "    def forward(self, clouds):\n",
    "        new_clouds = []\n",
    "        # Batch data check\n",
    "        assert len(clouds.size()) > 2, \"make sure data is batched version\"\n",
    "        B = clouds.size()[0]\n",
    "        N = clouds.size()[1]\n",
    "\n",
    "        batch = (torch.arange (0, B*N) // N).contiguous()\n",
    "        clouds = clouds.view(-1, 3).float().contiguous()\n",
    "        fps_cloud_indices = fps(clouds, batch, ratio=self.ratio)\n",
    "        return clouds[fps_cloud_indices, :].view(B, -1, 3).float()\n",
    "\n",
    "class kNN(nn.Module):\n",
    "    def __init__(self,k):\n",
    "        super().__init__()\n",
    "        assert k > 0\n",
    "        self.k = k\n",
    "    def forward(self, full_cloud, centers):\n",
    "        \"\"\"\n",
    "            Full_cloud : Full cloud\n",
    "            Centers : Coordinates of picked centers\n",
    "        \"\"\"\n",
    "        assert len(full_cloud) == len(centers), \"Both need to be of full size\"\n",
    "        assert len(full_cloud.size()) > 2, \"make sure data is batched version\"\n",
    "        assert len(centers.size()) > 2, \"make sure data is batched version\"\n",
    "        \n",
    "        patch_clouds = []\n",
    "        center_clouds = []\n",
    "\n",
    "        B_x = full_cloud.size()[0]\n",
    "        N_x = full_cloud.size()[1]\n",
    "\n",
    "        B_y = centers.size()[0]\n",
    "        N_y = centers.size()[1]\n",
    "\n",
    "        batch_x = (torch.arange (0, B_x*N_x) // N_x).contiguous()\n",
    "        batch_y = (torch.arange (0, B_y*N_y) // N_y).contiguous()\n",
    "\n",
    "        full_cloud = full_cloud.view(-1, 3).float().contiguous()\n",
    "        centers = centers.view(-1, 3).float().contiguous()\n",
    "\n",
    "        assign_indices = knn(full_cloud, centers, batch_x = batch_x, batch_y=batch_y, k=self.k)\n",
    "        x = full_cloud[assign_indices[1], :]\n",
    "        x = x.view(B_x, -1, self.k, 3).float()\n",
    "        \n",
    "        # Add centers to the already selected neighbors for each patch\n",
    "        c = centers.view(B_y, -1, 1, 3).float()\n",
    "        x = torch.cat([x, c], 2)\n",
    "        x = x - c\n",
    "        return x, c\n",
    "\n",
    "# THIS ONE WILL BE RANDOM MASKING\n",
    "# Create a masking layer which performs the operations of randomized masking of certain tokens\n",
    "class RandMasking(nn.Module):\n",
    "    def __init__(self, ratio, batch_size, patch_points):\n",
    "        super().__init__()\n",
    "        assert ratio > 0 and ratio <= 1\n",
    "        self.mask_ratio = ratio\n",
    "\n",
    "        # define a special learnable shared weighted mask token\n",
    "        self.mask_token = torch.nn.Parameter(torch.zeros((patch_points, 3)))\n",
    "    \n",
    "    def forward(self, patches):\n",
    "        assert len(patches.size()) > 2, \"make sure data is batched version\"\n",
    "\n",
    "        # create random indices to apply patch to\n",
    "        len_ = patches.size()[1]\n",
    "        indcs = np.arange(len_)\n",
    "        np.random.shuffle(indcs)\n",
    "        total = int(self.mask_ratio*len_)\n",
    "        mask_indcs = indcs[:total]\n",
    "        valid_token_indcs = indcs[total:]\n",
    "\n",
    "        valid_tokens = patches[:, valid_token_indcs, :, :]\n",
    "        masked_tokens = patches[:, mask_indcs, :, :]\n",
    "        valid_mask_patches = patches[:, mask_indcs, :, :] # Return the actual mask values to serve as output values\n",
    "        for b in range(masked_tokens.size()[0]):\n",
    "            for i in range(len(mask_indcs)):\n",
    "                masked_tokens[b,i,:,:] = self.mask_token\n",
    "        return valid_tokens, masked_tokens, valid_mask_patches, mask_indcs, valid_token_indcs\n",
    "        \n",
    "# Create a separate masking : To remove certain object complete parts instead of random masking\n",
    "# that way we'll be able to check how well symmetry is playing a role in helping the model learn inherant geometry\n",
    "class SpecialMasking(nn.Module):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A helper block for positional embeddings of the patches.\n",
    "class PE_MLP(nn.Module):\n",
    "    def __init__(self, embedding_dim, input_dim = 3):\n",
    "        super().__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.input_dim = input_dim\n",
    "        self.mid_units = 128\n",
    "\n",
    "        self.norm = nn.LayerNorm(normalized_shape = self.embedding_dim)\n",
    "        self.DLL = nn.Sequential(\n",
    "            nn.Linear(in_features = self.input_dim, out_features = self.mid_units),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(in_features = self.mid_units, out_features = self.embedding_dim),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.norm(self.DLL(x))\n",
    "\n",
    "\n",
    "### Define the complete Patch Embedding Layer encompassing all operations\n",
    "class Embedding(nn.Module):\n",
    "    def __init__(self, fps_ratio, batch_size, k_nn_k, msk_ratio, embed_dim, num_patches):\n",
    "        super().__init__()\n",
    "        self.fps = FPS(fps_ratio)\n",
    "        self.knn = kNN(k_nn_k-1) # Number of neighbors excluding center\n",
    "        self.masking_module = RandMasking(msk_ratio, batch_size, k_nn_k)\n",
    "        self.flatt_op = nn.Flatten(2,3) # Layer to flatten points in a given patch for a point cloud\n",
    "\n",
    "        # separate MLP blocks for encoder and decoder\n",
    "        self.encoder_MLP = PE_MLP(embed_dim, k_nn_k*3)\n",
    "        self.decoder_MLP = PE_MLP(embed_dim, k_nn_k*3)\n",
    "\n",
    "        # Also create another MLP block for centers\n",
    "        self.centers_embedding = PE_MLP(embed_dim, 3)\n",
    "    \n",
    "    def forward(self, inp):\n",
    "        c = self.fps(inp)\n",
    "        patches, centers = self.knn(inp, c)\n",
    "        valid_patches, masked_patches, valid_mask_patches, mask_indcs, val_indcs = self.masking_module(patches)\n",
    "        \n",
    "        # Flatten the valid patches and masked patches (x,y,z) coordinaters\n",
    "        valid_patches = self.flatt_op(valid_patches)\n",
    "        masked_patches = self.flatt_op(masked_patches)\n",
    "        \n",
    "        # masked_patches = self.flatt_op(masked_patches)\n",
    "        valid_patches = self.encoder_MLP(valid_patches)\n",
    "        masked_patches = self.decoder_MLP(masked_patches)\n",
    "\n",
    "        valid_center_embeddings = self.centers_embedding(centers[:,val_indcs,:,:]).flatten(2,3)\n",
    "        masked_center_embeddings = self.centers_embedding(centers[:,mask_indcs,:,:]).flatten(2,3)\n",
    "        \n",
    "        valid_patches = valid_patches + valid_center_embeddings\n",
    "        masked_patches = masked_patches + masked_center_embeddings\n",
    "\n",
    "        return valid_patches, masked_patches, valid_mask_patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the Transformer Encoder-Decoder (with prediction head as well)\n",
    "\n",
    "# Feed-forward block part of encoder and decoders\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, embedding_dim, units):\n",
    "        super().__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.units = units\n",
    "        self.DLL = nn.Sequential(\n",
    "            nn.Linear(in_features = self.embedding_dim, out_features = self.units),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(in_features = self.units, out_features = self.embedding_dim),\n",
    "            nn.GELU()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.DLL(x)\n",
    "\n",
    "class SelfAttentionBlock(nn.Module):\n",
    "    def __init__(self, embedding_dim, num_heads):\n",
    "        super().__init__()        \n",
    "        self.num_heads = num_heads\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        # self.norm = nn.LayerNorm(embedding_dim)\n",
    "        self.attention_heads = nn.MultiheadAttention(num_heads=num_heads, embed_dim=embedding_dim, dropout=0.2, batch_first=True)\n",
    "\n",
    "    def forward(self,inp):\n",
    "        # x = self.norm(inp)\n",
    "        x = inp\n",
    "        out,_ = self.attention_heads(query=x, key=x, value=x, need_weights=False)\n",
    "        return out\n",
    "\n",
    "class CrossAttentionBlock(nn.Module):\n",
    "    def __init__(self, embedding_dim, num_heads):\n",
    "        super().__init__()        \n",
    "        self.num_heads = num_heads\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.attention_heads = nn.MultiheadAttention(num_heads=num_heads, embed_dim=embedding_dim, dropout=0.2, batch_first=True)\n",
    "\n",
    "    def forward(self, k, v, q):\n",
    "        # Query will come from Masked patches and key & value will come from encoder\n",
    "        out,_ = self.attention_heads(query=q, key=k, value=v, need_weights=False)\n",
    "        return out\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, embedding_dim, mlp_units, num_heads):\n",
    "        super().__init__()\n",
    "        self.attention = SelfAttentionBlock(embedding_dim, num_heads=num_heads)\n",
    "        self.dense = FeedForward(embedding_dim, mlp_units)\n",
    "        self.norm = nn.LayerNorm(normalized_shape = embedding_dim)\n",
    "    \n",
    "    def forward(self,inp):\n",
    "        x = self.norm(self.attention(inp)+inp)\n",
    "        return self.norm(self.dense(x) + x)\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, embedding_dim, mlp_units, num_heads):\n",
    "        super().__init__()\n",
    "        self.cross_attention = CrossAttentionBlock(embedding_dim, num_heads=num_heads)\n",
    "        \n",
    "        self.dense = FeedForward(embedding_dim, mlp_units)\n",
    "        self.norm = nn.LayerNorm(normalized_shape = embedding_dim)\n",
    "    \n",
    "    def forward(self, decod_inp, encod_inp):\n",
    "        x = self.cross_attention( encod_inp, encod_inp, decod_inp)\n",
    "        return self.norm(self.dense(x) + x)\n",
    "\n",
    "class PT_MAE(nn.Module):\n",
    "    def __init__(self, fps_ratio, batch_size, k_nn_k, msk_ratio, embed_dim, num_patches, mlp_units, num_heads, encoder_depth, device):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.emb = Embedding(fps_ratio, batch_size, k_nn_k, msk_ratio, embed_dim, num_patches)\n",
    "        \n",
    "        self.Encoders = nn.Sequential(*[Encoder(embed_dim, mlp_units, num_heads) for i in range(encoder_depth)]).to(device)\n",
    "\n",
    "        # NOTE : For decoder block use much lower number of layers\n",
    "        # self.Decoders = nn.Sequential(*[Decoder(embed_dim, mlp_units, num_heads) for i in range(encoder_depth//2)])\n",
    "        \n",
    "        self.D1 = Decoder(embed_dim, mlp_units, num_heads).to(device)\n",
    "        self.D2 = Decoder(embed_dim, mlp_units, num_heads).to(device)\n",
    "        self.D3 = Decoder(embed_dim, mlp_units, num_heads).to(device)\n",
    "        self.D4 = Decoder(embed_dim, mlp_units, num_heads).to(device)\n",
    "\n",
    "        # Create a prediction head\n",
    "        self.Pred_Head = nn.Linear(embed_dim, 3*k_nn_k).to(device)\n",
    "    \n",
    "    def forward(self, inp):\n",
    "        valid_patches, masked_patches, valid_mask_patches = self.emb(inp)\n",
    "        \n",
    "        valid_patches = valid_patches.to(self.device)\n",
    "        masked_patches = masked_patches.to(self.device)\n",
    "        valid_mask_patches = valid_mask_patches.to(self.device)\n",
    "        x = self.Encoders(valid_patches)\n",
    "        decoded = self.D2(self.D1(masked_patches, x), x)\n",
    "        decoded = self.D3(decoded, x)\n",
    "        decoded = self.D4(decoded, x)\n",
    "\n",
    "        out = self.Pred_Head(decoded)\n",
    "        return out, valid_mask_patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, fps_ratio, batch_size, k_nn_k, msk_ratio, embed_dim, num_patches, mlp_units, num_heads, encoder_depth, epochs, model_path, device):\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "        self.device = device\n",
    "        self.learning_rate = 0.0002\n",
    "        self.model_path = model_path\n",
    "\n",
    "        # Check if model_path is non empty/present\n",
    "        if os.path.exists(model_path):\n",
    "            print(\"Loading Existing model\")\n",
    "            self.pt_mae = torch.load(model_path)\n",
    "        else:\n",
    "            self.pt_mae = PT_MAE(fps_ratio, batch_size, k_nn_k, msk_ratio, embed_dim, num_patches, mlp_units, num_heads, encoder_depth, device = self.device)\n",
    "        # self.pt_mae = self.pt_mae.to(self.device)\n",
    "\n",
    "        # Setup optimizers\n",
    "        self.optimizer = optim.Adam(self.pt_mae.parameters(), lr=self.learning_rate)\n",
    "\n",
    "    def profile_model(self,):\n",
    "        with profile(activities=[ProfilerActivity.CPU], record_shapes=True) as prof:\n",
    "            with h5py.File(\"../Data/Mini-PointNet.h5py\", \"r\") as f:\n",
    "                inputs = torch.from_numpy(f[\"data\"][\"Chair\"][:40].astype(np.float32))\n",
    "                with record_function(\"model_inference\"):\n",
    "                    self.pt_mae(inputs)\n",
    "        print(prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=12))\n",
    "    \n",
    "    def train(self, dataset_file_path = \"../Data/Mini-PointNet.h5py\", epochs = None):\n",
    "        # Prepare data for training\n",
    "        data = []\n",
    "        with h5py.File(dataset_file_path, \"r\") as f:\n",
    "            for c in (\"Table\", \"Chair\", \"Sofa:Couch\", \"Lamp\"):\n",
    "                data.append(f[\"data\"][c][:].astype(np.float32))\n",
    "        data = np.vstack(data)\n",
    "        np.random.shuffle(data)\n",
    "\n",
    "        train_dataset = TensorDataset(torch.from_numpy(data))\n",
    "        train_loader = DataLoader(dataset = train_dataset, batch_size = self.batch_size, shuffle=True)\n",
    "\n",
    "        if epochs is not None:\n",
    "            epochs = epochs\n",
    "        else:\n",
    "            epochs = self.epochs\n",
    "        \n",
    "        for epoch in tqdm(range(1, epochs + 1)):\n",
    "            for batch_idx, data in enumerate(train_loader):\n",
    "                data = data[0]\n",
    "                out, valid_mask_patches = self.pt_mae(data)\n",
    "                valid_mask_patches = valid_mask_patches.flatten(2,3)\n",
    "                loss = my_chamfer_dist(out, valid_mask_patches)\n",
    "\n",
    "                if batch_idx % 600 == 0:\n",
    "                    print(f\"Existing Loss in Epoch : {epoch}, Batch : {batch_idx} ===>\", round(loss.item(),3))\n",
    "                \n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "            \n",
    "            # Save model after every 2 epochs\n",
    "            if epoch % 2 == 0:\n",
    "                torch.save(self.pt_mae,self.model_path)\n",
    "        # save the model at the end also\n",
    "        torch.save(self.pt_mae,self.model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Existing model\n"
     ]
    }
   ],
   "source": [
    "m = Model(FPS_RATIO, BATCH_SIZE, K_NN_K, MASK_RATIO, EMBEDDING_DIM, NUM_PATCHES, MLP_UNITS, NUM_HEADS, ENCODER_DEPTH, EPOCHS, \"./Saved_Model2\", device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3feaca7d11114c81b1d553a8bcd64549",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existing Loss in Epoch : 1, Batch : 0 ===> 12.684\n",
      "Existing Loss in Epoch : 2, Batch : 0 ===> 13.159\n",
      "Existing Loss in Epoch : 3, Batch : 0 ===> 12.762\n",
      "Existing Loss in Epoch : 4, Batch : 0 ===> 12.994\n",
      "Existing Loss in Epoch : 5, Batch : 0 ===> 12.618\n",
      "Existing Loss in Epoch : 6, Batch : 0 ===> 12.835\n",
      "Existing Loss in Epoch : 7, Batch : 0 ===> 13.437\n",
      "Existing Loss in Epoch : 8, Batch : 0 ===> 12.905\n",
      "Existing Loss in Epoch : 9, Batch : 0 ===> 12.076\n",
      "Existing Loss in Epoch : 10, Batch : 0 ===> 12.323\n",
      "Existing Loss in Epoch : 11, Batch : 0 ===> 13.569\n",
      "Existing Loss in Epoch : 12, Batch : 0 ===> 12.517\n",
      "Existing Loss in Epoch : 13, Batch : 0 ===> 12.696\n",
      "Existing Loss in Epoch : 14, Batch : 0 ===> 11.84\n",
      "Existing Loss in Epoch : 15, Batch : 0 ===> 12.147\n",
      "Existing Loss in Epoch : 16, Batch : 0 ===> 12.459\n",
      "Existing Loss in Epoch : 17, Batch : 0 ===> 12.108\n",
      "Existing Loss in Epoch : 18, Batch : 0 ===> 11.603\n",
      "Existing Loss in Epoch : 19, Batch : 0 ===> 12.456\n",
      "Existing Loss in Epoch : 20, Batch : 0 ===> 13.108\n"
     ]
    }
   ],
   "source": [
    "m.train(epochs = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if platform == \"darwin\":\n",
    "    os.system(\"say 'Traning Epochs Completed!'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
