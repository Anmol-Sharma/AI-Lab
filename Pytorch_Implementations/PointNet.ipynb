{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "from sys import platform\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device in use : mps\n"
     ]
    }
   ],
   "source": [
    "device = \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "torch.set_default_device(device)\n",
    "print(\"Device in use :\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 24\n",
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pcs = np.load(\"../data/ModelNet10/train_pc.npz\")\n",
    "test_pcs = np.load(\"../data/ModelNet10/test_pc.npz\")\n",
    "\n",
    "train_data, train_labels = train_pcs['train_pc'], train_pcs[\"train_labels\"]\n",
    "test_data, test_labels = test_pcs['test_pc'], test_pcs[\"test_labels\"]\n",
    "\n",
    "# Lookup table to encode Labels to be consumed by the model\n",
    "lookup = {\n",
    "    \"bed\": [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
    "    \"bathtub\": [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
    "    \"chair\": [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
    "    \"desk\": [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
    "    \"dresser\": [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
    "    \"monitor\": [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
    "    \"night_stand\": [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
    "    \"sofa\": [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
    "    \"table\": [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    \"toilet\": [1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "}\n",
    "\n",
    "train_labels = np.array([lookup[i] for i in train_labels]).astype(np.float32)\n",
    "test_labels = np.array([lookup[i] for i in test_labels]).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomMax(nn.Module):\n",
    "    def forward(self, inp):\n",
    "        return torch.max(inp, dim = 1).values\n",
    "\n",
    "class PointNet(nn.Module):\n",
    "    def __init__(self, num_points, batch_size, channels = 3, n_classes = 10):\n",
    "        super().__init__()\n",
    "        self.channels = channels\n",
    "        self.n_classes = n_classes\n",
    "        self.num_points = num_points\n",
    "        self.batch_size = batch_size\n",
    "        self.model = nn.Sequential(\n",
    "            self.__dense_layer_op(),\n",
    "            self.__dense_layer_op(32, 32),\n",
    "            self.__dense_layer_op(32, 32),\n",
    "            self.__dense_layer_op(32, 64),\n",
    "            self.__dense_layer_op(64, 512),\n",
    "            # At this point we have batch_size x 2048 x 512 features\n",
    "            # We want batch_size x 512 feature ahead of this point\n",
    "            # So we want to perform global max pooling over the 2048 x 512 set for each example.\n",
    "            CustomMax(),\n",
    "            self.__dense_layer_op(512, 256, over_out_channel=True),\n",
    "            nn.Dropout(0.3),\n",
    "            self.__dense_layer_op(256, 128, over_out_channel=True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, self.n_classes),\n",
    "            nn.Softmax()\n",
    "        )\n",
    "    \n",
    "    def __dense_layer_op(self, units_in = 3, units_out = 32, over_out_channel = False):\n",
    "        if over_out_channel:\n",
    "            return nn.Sequential(\n",
    "            nn.Linear(units_in, units_out),\n",
    "            nn.BatchNorm1d(units_out, momentum=0.7),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "        \n",
    "        return nn.Sequential(\n",
    "            nn.Linear(units_in, units_out),\n",
    "            nn.BatchNorm1d(self.num_points, momentum=0.7),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "    \n",
    "    def forward(self, inp):\n",
    "        return self.model(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PtNet_Model:\n",
    "    def __init__(self, num_points, n_classes, batch_size, epochs, device):\n",
    "        self.num_points = num_points\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "        self.n_classes = n_classes\n",
    "        self.learning_rate=0.002\n",
    "        self.device = device\n",
    "        self.channels = 4\n",
    "\n",
    "        self.PtNet = PointNet(self.num_points, self.channels, self.n_classes).to(self.device)\n",
    "\n",
    "        # Setup optimizers\n",
    "        self.optimizer = optim.Adam(self.PtNet.parameters(), lr=self.learning_rate)\n",
    "\n",
    "        # Setup loss function\n",
    "        self.loss_fn = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    def train(self,):\n",
    "        global train_data\n",
    "        global train_labels\n",
    "\n",
    "        train_dataset = TensorDataset(torch.from_numpy(train_data), torch.from_numpy(train_labels))\n",
    "        idcs = np.arange(len(train_data))\n",
    "        np.random.shuffle(idcs)\n",
    "        validation_dataset = torch.utils.data.Subset(train_dataset, idcs[:350])\n",
    "\n",
    "        train_loader = DataLoader(dataset = train_dataset, batch_size = self.batch_size, shuffle=True, generator=torch.Generator(device=self.device), pin_memory=True)\n",
    "        # train_loader = DataLoader(dataset = train_dataset, batch_size = self.batch_size, shuffle=True)\n",
    "        validation_loader = DataLoader(dataset = validation_dataset, shuffle=True, batch_size = self.batch_size, generator=torch.Generator(device=self.device), pin_memory=True)\n",
    "\n",
    "        for epoch in tqdm(range(1, self.epochs + 1)):\n",
    "            for batch_idx, (data, labels) in enumerate(train_loader):\n",
    "                data, labels = data.to(self.device), labels.to(self.device)\n",
    "                model_labels = self.PtNet(data)\n",
    "                loss = self.loss_fn(model_labels, labels)\n",
    "                \n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                # if batch_idx % 100==0:\n",
    "                #     print(\"Loss :\", loss.item())\n",
    "\n",
    "            # Perform validation operation\n",
    "            with torch.no_grad():\n",
    "                correct, total = 0, 0\n",
    "                for batch_idx, (data, labels) in enumerate(validation_loader):\n",
    "                    data, labels = data.to(self.device), labels.to(self.device)\n",
    "                    outputs = self.PtNet(data)\n",
    "                    loss = self.loss_fn(outputs, labels)\n",
    "                    predicted = torch.argmax(outputs.data, dim=1)\n",
    "                    labels = torch.argmax(labels, dim=1)\n",
    "                    \n",
    "                    total += labels.size(0)\n",
    "                    correct += (predicted == labels).sum().item()\n",
    "                print(f'Validation Accuracy of the network : {100 * correct // total}% and validation loss : {np.round(loss.item(), 3)}')\n",
    "    def test(self,):\n",
    "        global test_data\n",
    "        global test_labels\n",
    "\n",
    "        test_dataset = TensorDataset(torch.from_numpy(test_data), torch.from_numpy(test_labels))\n",
    "        test_loader = DataLoader(dataset=test_dataset, shuffle=True, batch_size=100)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            correct, total = 0, 0\n",
    "            loss = 0.0\n",
    "            for batch_idx, (data, labels) in enumerate(test_loader):\n",
    "                data, labels = data.to(self.device), labels.to(self.device)\n",
    "                outputs = self.PtNet(data)\n",
    "                loss = self.loss_fn(outputs, labels)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                _, labels = torch.max(labels, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "            print(f'Test Accuracy of the network : {100 * correct // total}% and test loss : {np.round(loss.item(), 3)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_net = PtNet_Model(2048, 10, BATCH_SIZE, EPOCHS, device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c816571d038439ea8913f6399a1b24e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anmol/Codebase/Environments/AI/lib/python3.11/site-packages/torch/nn/functional.py:1881: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return handle_torch_function(softmax, (input,), input, dim=dim, _stacklevel=_stacklevel, dtype=dtype)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy of the network : 71% and validation loss : 1.81\n",
      "Validation Accuracy of the network : 73% and validation loss : 1.756\n",
      "Validation Accuracy of the network : 75% and validation loss : 1.627\n",
      "Validation Accuracy of the network : 77% and validation loss : 1.889\n",
      "Validation Accuracy of the network : 78% and validation loss : 1.585\n",
      "Validation Accuracy of the network : 78% and validation loss : 1.728\n",
      "Validation Accuracy of the network : 78% and validation loss : 1.583\n",
      "Validation Accuracy of the network : 77% and validation loss : 1.726\n",
      "Validation Accuracy of the network : 81% and validation loss : 1.799\n",
      "Validation Accuracy of the network : 80% and validation loss : 1.659\n"
     ]
    }
   ],
   "source": [
    "pt_net.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if platform == \"darwin\":\n",
    "    os.system(\"say 'Model Training DONE!'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI_ENV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
